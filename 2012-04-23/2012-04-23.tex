\documentclass{article}
\begin{document}
I ran this only for the 23 April snapshot;
this doesn't give performance indicators
over time. As such, this is probably more
useful as an idea of things to extract from
the runevents and user data as potential
performance indicator variables.

I haven't come up with a way to aggregate
the graphs into one useful and non-arbitrary
number. An easy way to do that might be to
build some model to predict what users are like.

For example, we could select a random sample of
42 users from the users with at least six scripts
and who had registered at least a month ago.
We could contact all of these users and have
them do things.
\begin{itemize}
\item If we want to predict pretty much anything,
  we could start with a questionnaire or web search.
\item If we want to predict coding ability, we
  could provide the users with clear specifications
  for a scraper and tell them that we are interested
  in hiring them for the job. We can derive numbers
  from the quality of the work, the time of completion,
  the cost of the work, \&c.
\item We might really want to predict interest in
  purchasing, but we don't currently have enough data for
  that because few people own vaults. We could email
  each of these users offering a substantially discounted
  premium account plan and see whether they buy.
\end{itemize}
Based on these results, we can try predicting characteristics
of the full population of users with at least six scripts
who had registered at least a month ago.
\end{document}
